{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Network.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPuhf/gyfc/IMi2cTnCUiED",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audrey-siqueira/Data-Science-Projects/blob/master/Convolutional_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r89jL6gYsyyg"
      },
      "source": [
        "# **Dog/Cat Image Labeling using Convolutional Neural Network**\n",
        "---\n",
        "<p align=\"justify\">\n",
        "In this project, the task is to develop an algorithm to classify images of dogs and cats, which relies on the problem of distinguishing images of dogs and cats. It is easy for humans, but evidence suggests that cats and dogs are particularly difficult to tell apart automatically.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://drive.google.com/uc?id=1OQN5FNG3ub5-BFe-LSj5y7ZSA3d9i3wj\" width=\"50%\"></p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "The input for this task is images of dogs or cats from training dataset, while the output is the classification accuracy on test dataset. the training set contains 8000 images, including 4000 images of dogs and 4000 images of cats,\n",
        "while the test dataset contains 2000 images.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://drive.google.com/uc?id=128FUIGWqHdCijr8ltyCp-aiChMjGYajv\" width=\"80%\"></p>\n",
        "\n",
        "\n",
        "The strategy in this case is to use the **Convolutional Neural Network** to learn the image patterns and return a label for each image if is a dog or a cat. \n",
        "\n",
        "\n",
        "**Code description is explained below:**\n",
        "\n",
        "**In this case it is required upload the database direct from local computer and not import from google drive, because the process become slower using drive.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E0Q3aoKUCRX"
      },
      "source": [
        "## **Part 1 - Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2LGDnFTDFk"
      },
      "source": [
        "### **Importing the libraries**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74m--yTF9AzV"
      },
      "source": [
        "The 2 libraries needed for the project are imported.\n",
        "- Tensorflow and Keras for Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bru5xErJTFNt",
        "outputId": "27cdcad2-6844-481b-d7fd-eed055bd34c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRyWwQFi14X-",
        "outputId": "70649a51-dece-4b91-d533-7597f7e251b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip original.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open original.zip, original.zip.zip or original.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### **Preprocessing the Training set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0koUcJMJpEBD",
        "outputId": "a0692472-73d8-427f-e4c7-0e2615f1ee76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('/content/dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8c91517c267e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                  \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                  class_mode = 'binary')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m   def flow_from_dataframe(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset/training_set'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### **Preprocessing the Test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH4WzfOhpKc3"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## **Part 2 - Building the CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### **Initialising the CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAUt4UMPlhLS"
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### **Convolution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPzPrMckl-hV"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### **Pooling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncpqPl69mOac"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### **Adding a second convolutional layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-FZjn_m8gk"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### **Flattening**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZeOGCvnNZn"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### **Full Connection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GtmUlLd26Nq"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### **Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_Zj1Mc3Ko_"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## **Part 3 - Training the CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### **Compiling the CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NALksrNQpUlJ"
      },
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### **Training the CNN on the Training set and evaluating it on the Test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUj1W4PJptta",
        "outputId": "ae555643-d0ef-4799-c5f9-ef87eee8a0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.6590 - accuracy: 0.5987 - val_loss: 0.6060 - val_accuracy: 0.6710\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.5878 - accuracy: 0.6842 - val_loss: 0.5662 - val_accuracy: 0.7080\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 30s 122ms/step - loss: 0.5508 - accuracy: 0.7139 - val_loss: 0.5223 - val_accuracy: 0.7440\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 31s 123ms/step - loss: 0.5143 - accuracy: 0.7446 - val_loss: 0.5572 - val_accuracy: 0.7165\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.4918 - accuracy: 0.7619 - val_loss: 0.4957 - val_accuracy: 0.7560\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 31s 123ms/step - loss: 0.4759 - accuracy: 0.7688 - val_loss: 0.4914 - val_accuracy: 0.7520\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.4574 - accuracy: 0.7837 - val_loss: 0.4788 - val_accuracy: 0.7695\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.4462 - accuracy: 0.7895 - val_loss: 0.4644 - val_accuracy: 0.7810\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 31s 122ms/step - loss: 0.4345 - accuracy: 0.7986 - val_loss: 0.4546 - val_accuracy: 0.7850\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 31s 122ms/step - loss: 0.4192 - accuracy: 0.8046 - val_loss: 0.4500 - val_accuracy: 0.7945\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 30s 121ms/step - loss: 0.4092 - accuracy: 0.8105 - val_loss: 0.4345 - val_accuracy: 0.8100\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 30s 122ms/step - loss: 0.3936 - accuracy: 0.8194 - val_loss: 0.4356 - val_accuracy: 0.8025\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 30s 121ms/step - loss: 0.3833 - accuracy: 0.8281 - val_loss: 0.4406 - val_accuracy: 0.8025\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 30s 120ms/step - loss: 0.3708 - accuracy: 0.8305 - val_loss: 0.4430 - val_accuracy: 0.8015\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 31s 125ms/step - loss: 0.3706 - accuracy: 0.8282 - val_loss: 0.4277 - val_accuracy: 0.8120\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 31s 122ms/step - loss: 0.3643 - accuracy: 0.8379 - val_loss: 0.4177 - val_accuracy: 0.8100\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 31s 122ms/step - loss: 0.3561 - accuracy: 0.8395 - val_loss: 0.4175 - val_accuracy: 0.8115\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.3421 - accuracy: 0.8441 - val_loss: 0.4225 - val_accuracy: 0.8155\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 31s 123ms/step - loss: 0.3304 - accuracy: 0.8524 - val_loss: 0.4291 - val_accuracy: 0.8155\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.3264 - accuracy: 0.8524 - val_loss: 0.4626 - val_accuracy: 0.8085\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.3196 - accuracy: 0.8570 - val_loss: 0.4369 - val_accuracy: 0.8135\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 31s 124ms/step - loss: 0.3077 - accuracy: 0.8629 - val_loss: 0.4417 - val_accuracy: 0.8115\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 31s 125ms/step - loss: 0.3017 - accuracy: 0.8650 - val_loss: 0.4303 - val_accuracy: 0.8145\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 31s 125ms/step - loss: 0.2898 - accuracy: 0.8716 - val_loss: 0.4448 - val_accuracy: 0.8140\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 32s 130ms/step - loss: 0.2877 - accuracy: 0.8733 - val_loss: 0.4438 - val_accuracy: 0.8115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb3757c8128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## **Part 4 - Making a single prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsSiWEJY1BPB",
        "outputId": "e08d0ccb-96f4-486d-c958-e83d29cc1705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        " \n",
        "test_image = image.load_img('/content/dataset/test_set/cats/cat.4019.jpg', target_size=(64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image=test_image/255\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        " \n",
        "results=cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if results[0][0] >= 0.5:\n",
        "    print(\"Dog\")\n",
        "else:\n",
        "    print(\"Cat\")\t\t\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gato\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEGUbHaI5-xR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}